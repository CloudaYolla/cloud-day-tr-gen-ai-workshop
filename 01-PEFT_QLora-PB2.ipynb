{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43e1ffa3-aa43-4fba-97d6-30d2554d7957",
   "metadata": {},
   "source": [
    "## Domain specific fine-tuning Falcon-7B model using QLoRA, HF PEFT and bitsandbytes\n",
    "\n",
    "Fine-tuning large language models (LLMs) allows you to adjust open-source foundational models to achieve improved performance on your domain-specific tasks. In this notebook, we will see how we can leverage Amazon SageMaker to fine-tune state-of-the-art open-source model Falcon-7B. We utilize Hugging Face’s parameter-efficient fine-tuning (PEFT) library and quantization techniques through bitsandbytes to support fine-tuning of extremely large model. We will be using a new technique known as Quantized LLMs with Low-Rank Adapters (QLoRA). QLoRA is an efficient fine-tuning approach that reduces memory usage of LLMs while maintaining solid performance.\n",
    "\n",
    "For the purpose of this lab, we will be using [Medical Q&A](https://huggingface.co/datasets/medalpaca/medical_meadow_medical_flashcards) dataset from HuggingFace.\n",
    "\n",
    "> **Note** This link leads to a Third-Party Dataset. AWS does not own, nor does it have any control over the Third-Party Dataset. You should perform your own independent assessment, and take measures to ensure that you comply with your own specific quality control practices and standards, and the local rules, laws, regulations, licenses and terms of use that apply to you, your content, and the Third-Party Dataset. AWS does not make any representations or warranties that the Third-Party Dataset is secure, virus-free, accurate, operational, or compatible with your own environment and standards. AWS does not make any representations, warranties or guarantees that any information in the Third-Party Dataset will result in a particular outcome or result.\n",
    "\n",
    "This notebook was tested in Amazon SageMaker Studio with `Python 3 (Data Science 3.0)` kernel and a `ml.g5.12xlarge` instance, and in an Amazon SageMaker Notebook instance with `conda_python3` kernel in a `ml.g5.12xlarge` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20b441b-8c77-4ccc-8050-9bb5315a2263",
   "metadata": {},
   "source": [
    "Install the required libriaries. To load the model in 4-bit, install the Hugging Face libraries including accelerate, transformers, and PEFT from source, as well as the latest version of bitsandbytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac2a4c-8725-4856-a526-a9a84bb7bf42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env PIP_DISABLE_PIP_VERSION_CHECK True\n",
    "%env PIP_ROOT_USER_ACTION ignore\n",
    "\n",
    "%pip install -q -U torch==2.0.1 bitsandbytes==0.39.1 --root-user-action=ignore\n",
    "%pip install -q -U datasets py7zr einops tensorboardX --root-user-action=ignore\n",
    "%pip install -q -U git+https://github.com/huggingface/transformers.git@850cf4af0ce281d2c3e7ebfc12e0bc24a9c40714 --root-user-action=ignore\n",
    "%pip install -q -U git+https://github.com/huggingface/peft.git@e2b8e3260d3eeb736edf21a2424e89fe3ecf429d --root-user-action=ignore \n",
    "%pip install -q -U git+https://github.com/huggingface/accelerate.git@b76409ba05e6fa7dfc59d50eee1734672126fdba --root-user-action=ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2d3d9c",
   "metadata": {},
   "source": [
    "Restart the kernel before proceeding to take effect of new installations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0654dc5",
   "metadata": {},
   "source": [
    "Next, we set the CUDA environment path using the installed CUDA that was a dependency of PyTorch installation. This is a required step for the bitsandbytes library to correctly find and load the correct CUDA shared object binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a57de0d-689f-406b-8ec3-c6d5e00894f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add installed cuda runtime to path for bitsandbytes\n",
    "import os\n",
    "import nvidia\n",
    "\n",
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee2a76-5240-4fba-aa51-348f6d2b212a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b6d520-613c-4a1b-8cf8-32ea9b624cff",
   "metadata": {},
   "source": [
    "To train our model, we need to convert our inputs (text) to token IDs. This is done by a Hugging Face Transformers Tokenizer. In addition to QLoRA, we will use bitsanbytes 4-bit precision to quantize out frozen LLM to 4-bit and attach LoRA adapters on it. We use bitsandbytes to quantize the Falcon-7B model into 4-bit precision so that we can load the model into memory on 4 A10G GPUs using Hugging Face Accelerate’s native pipeline parallelism. QLoRA tuning is shown to match 16-bit fine-tuning methods in a wide range of experiments because model weights are stored as 4-bit NF4 (noramlized float 4), but are dequantized to the computation bfloat16 on forward and backward passes as needed. We are using NF4 based on recommendations from the QLoRA paper. \n",
    "\n",
    "Another option includes bnb_4bit_use_double_quant, which uses a second quantization after the first one to save an additional 0.4 bits per parameter. While 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen (float16, bfloat16, float32 etc) for compute.\n",
    "\n",
    "The matrix multiplication and training will be faster if one uses a 16-bit compute dtype (default torch.float32). We leverage the recent BitsAndBytesConfig from transformers to change these parameters. An example to load a model in 4-bit using NF4 quantization is below with double quantization with the compute dtype bfloat16 for faster training.\n",
    "\n",
    "When loading the pretrained weights, we specify device_map=”auto\" so that Hugging Face Accelerate will automatically determine which GPU to put each layer of the model on. This process is known as model parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd847b-4520-4389-9c4f-87e92e40cc3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id=\"tiiuae/falcon-7b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68147ace",
   "metadata": {},
   "source": [
    "With Hugging Face’s PEFT library, you can freeze most of the original model weights and replace or extend model layers by training an additional, much smaller, set of parameters. This makes training much less expensive in terms of required compute. We set the Falcon modules that we want to fine-tune as target_modules in the LoRA configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe859fb-772c-4470-a415-442501e4e2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "    \n",
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f2fbb6",
   "metadata": {},
   "source": [
    "Notice that we’re only fine-tuning a small percentage of the model’s parameters, which makes this feasible in a reasonable amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12cd63-9411-4d15-b539-41bf57d95048",
   "metadata": {},
   "source": [
    "### Loading dataset\n",
    "\n",
    "For the purpose of the demo, we will load only 20% of the dataset as training dataset and last 10% of dataset as evaluation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41174b5c-59b4-4d11-84c8-b8e3548d220b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "    # datasets.ReadInstruction('train', to=20, unit='%'),\n",
    "    # datasets.ReadInstruction('train', from_=-10, unit='%')\n",
    "\n",
    "dataset_split = [\n",
    "    datasets.ReadInstruction('train', to=10, unit='%'),\n",
    "    datasets.ReadInstruction('train', from_=-5, unit='%')\n",
    "]\n",
    "\n",
    "train_dataset, eval_dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\", split=dataset_split)\n",
    "print(train_dataset.shape)\n",
    "print(eval_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d06fed9",
   "metadata": {},
   "source": [
    "Create a prompt template and load the dataset with a random sample to try summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8f130-2895-4d2b-abce-5e9700829d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# custom instruct prompt start\n",
    "prompt_template = f\"Answer this question truthfully:\\n{{question}}\\n---\\nAnswer:\\n{{answer}}{{eos_token}}\"\n",
    "\n",
    "# template dataset to add prompt to each sample\n",
    "def template_dataset(sample):\n",
    "    sample[\"text\"] = prompt_template.format(question=sample[\"input\"],\n",
    "                                            answer=sample[\"output\"],\n",
    "                                            eos_token=tokenizer.eos_token)\n",
    "    return sample\n",
    "\n",
    "\n",
    "# apply prompt template per sample\n",
    "train_dataset = train_dataset.map(template_dataset, remove_columns=list(train_dataset.features))\n",
    "eval_dataset = eval_dataset.map(template_dataset, remove_columns=list(eval_dataset.features))\n",
    "\n",
    "print(train_dataset[randint(0, len(train_dataset))][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de396cb-aa73-43e3-a68b-c3f9568b23a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#tokenize and chunk dataset\n",
    "lm_train_dataset = train_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, batch_size=48, remove_columns=list(train_dataset.features)\n",
    ")\n",
    "\n",
    "lm_eval_dataset = eval_dataset.map(\n",
    "    lambda sample: tokenizer(sample[\"text\"]), batched=True, batch_size=48, remove_columns=list(eval_dataset.features)\n",
    ")\n",
    "\n",
    "# Print total number of samples\n",
    "print(f\"Total number of train samples: {len(lm_train_dataset)}\")\n",
    "print(f\"Total number of evaluation samples: {len(lm_eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de9a45c-d5cf-48d3-afe8-b4f198fbf0ac",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3148bf5f",
   "metadata": {},
   "source": [
    "Use the Hugging Face Trainer class to fine-tune the model. Define the hyperparameters we want to use. We also create a DataCollator that will take care of padding our inputs and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c505379c-d770-4954-931a-2e11c57d93a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "#set the Falcon tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# We set num_train_epochs=1 simply to run a demonstration\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=lm_train_dataset,\n",
    "    eval_dataset=lm_eval_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_steps=100,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=2e-4,\n",
    "        bf16=True, # For g5 instances\n",
    "        # fp16=True, # for g4 instances\n",
    "        save_strategy = \"no\",\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ada7db-9846-400e-adbb-d54fbb59ff9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7d989e-352e-423f-b188-ca7eb15660e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#evaluate and return the metrics\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d5d78-5aaa-453d-aaac-5e62a2bca4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save model to use it for inference\n",
    "model.save_pretrained(\"qlora-finetuned-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b699a2-c716-4b0f-a58c-10f6f8bd70ae",
   "metadata": {},
   "source": [
    "### Load saved adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f68f5-f2c6-4cfc-9ed3-f8f30064f549",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5d0c7-e084-41d5-97dd-c7abe856fe4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_model_id = \"qlora-finetuned-model\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a13298-4037-4e5f-a3fc-246b48197c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orig_model_id=\"tiiuae/falcon-7b\"\n",
    "orig_tokenizer = AutoTokenizer.from_pretrained(orig_model_id)\n",
    "orig_model = AutoModelForCausalLM.from_pretrained(orig_model_id, device_map=\"auto\", trust_remote_code=True) \n",
    "# quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ca284d-775e-449f-b999-134808af13b9",
   "metadata": {},
   "source": [
    "### Inference using fine-tuned model and original model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ae966",
   "metadata": {},
   "source": [
    "Set the hyperparameters for the LLM to perform inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b86fd5-0511-4c1e-be44-f586d08cd8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_config = model.generation_config\n",
    "generation_config.max_new_tokens = 50\n",
    "generation_config_temperature = 0.7\n",
    "generation_config.top_p = 0.7\n",
    "generation_config.num_return_sequences = 1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "generation_config_eod_token_id = tokenizer.eos_token_id\n",
    "generation_config.repetition_penalty = 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1bbd3-f497-4bd5-90a9-362c0be16c46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "orig_generation_config = orig_model.generation_config\n",
    "orig_generation_config.max_new_tokens = 50\n",
    "orig_generation_config_temperature = 0.7\n",
    "orig_generation_config.top_p = 0.7\n",
    "orig_generation_config.num_return_sequences = 1\n",
    "orig_generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "orig_generation_config_eod_token_id = tokenizer.eos_token_id\n",
    "orig_generation_config.repetition_penalty = 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d17d4",
   "metadata": {},
   "source": [
    "Interact with the model by using the following prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0332728-f6a0-4fa3-a318-50a964ed9d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = f\"\"\"\n",
    "Answer this question truthfully:\n",
    "What are the treatments for ARDS?\n",
    "---\n",
    "Answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        input_ids=encoding.attention_mask,\n",
    "        generation_config=generation_config,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea493bf-77ac-4e53-b2b9-d210d0d5e7e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_response(question, model):\n",
    "    prompt = f\"\"\"Answer this question truthfully:\n",
    "    {question}\n",
    "    ---\n",
    "    Answer:\n",
    "    \"\"\".strip()\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            input_ids=encoding.input_ids,\n",
    "            attention_mask=encoding.attention_mask,\n",
    "            generation_config=generation_config,\n",
    "        )\n",
    "    response = tokenizer.decode(outputs[0],skip_special_tokens=True)\n",
    "    \n",
    "    answer_start = 'Answer:'\n",
    "    response_start = response.find(answer_start)\n",
    "    return response[response_start + len(answer_start):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe6708c-5809-400d-a104-5b2851c8862f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What are the treatments for ARDS?\"\n",
    "print(generate_response(prompt, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038db87-f642-4d22-b95b-0611eb9599dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What are the treatments for ARDS?\"\n",
    "print(generate_response(prompt, orig_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ba7382-8be2-4a4a-8d2d-677c659b16d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What does low mobility and bulging of TM suggest?\"\n",
    "print(generate_response(prompt, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76ba6ff-b668-4a91-9f24-7eba9980f11f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What does low mobility and bulging of TM suggest?\"\n",
    "print(generate_response(prompt, orig_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984c059-fe94-4d8c-8aee-ccf48d1df781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.g5.12xlarge",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
